{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import jieba\n",
    "import logging\n",
    "import re\n",
    "import gensim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "jieba.setLogLevel(logging.INFO)#显示结巴分词日志\n",
    "\n",
    "regex = re.compile(r'[^\\u4e00-\\u9fa5aA-Za-z0-9]')#正则表达式，返回的是一个匹配对象，https://www.runoob.com/regexp/regexp-syntax.html\n",
    "\n",
    "# def word_cut(text):#切的结果为字符串\n",
    "#     text = regex.sub('', text)\n",
    "#     cut_str = ' '.join(jieba.cut(text, cut_all=False))#切为字符串并在词间穿插空格\n",
    "#     return cut_str\n",
    "\n",
    "def word_cut(text):#结果为list\n",
    "    text = regex.sub(' ', text)\n",
    "    return [word for word in jieba.cut(text) if word.strip()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%读取并整合数据\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集： [('操控性舒服、油耗低，性价比高', 1), ('动力的确有点点让我相信了up的确是个代步车而已!', 0), ('1。车的外观很喜欢。2。省油，现在磨合期7.3，相信以后还会下降。', 1), ('内饰的做工和用料同级别同价位最厚道的', 1), ('减震系统太硬！', 0)]\n",
      "--------------------------------------------------\n",
      "验证集： [('外观确实非常霸气，钻石切工有棱有角，3.0的动力在城市里绰绰有余，内饰考究，空间比较大，bose的音响非常给力，小众品牌不像德系三架马车那样成为街车，为个性代言。', 1), ('外观漂亮，安全性佳，动力够强，油耗够低', 1), ('后备箱大！！！', 1), ('空间大。外观大气，中控台用料讲究简洁', 1), ('外观漂亮，空间够大，动力家用也ok', 1)]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data/test.tsv', sep='\\t',)\n",
    "dev = pd.read_csv('data/dev.tsv', sep='\\t',)\n",
    "x_all = list(pd.read_csv(\"data/ch_auto.csv\")[\"text\"])\n",
    "#train.head(5)#查看数据样式\n",
    "\n",
    "train_x = train['text']#按行抽取\n",
    "train_y = train['label']\n",
    "dev_x = dev['text']\n",
    "dev_y = dev['label']\n",
    "\n",
    "train_dataset = []#将组合好的(x，y)放入此列表中\n",
    "dev_dataset = []\n",
    "\n",
    "for i in range(len(train_x)):#整合为训练数据集（初步）\n",
    "    train_dataset.append((train_x[i],train_y[i]))\n",
    "print(\"训练集：\",train_dataset[:5])\n",
    "print(\"-\"*50)\n",
    "for i in range(len(dev_x)):#整合为验证数据集（初步）\n",
    "    dev_dataset.append((dev_x[i],dev_y[i]))\n",
    "print(\"验证集：\",dev_dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#将训练集及验证集的x整合到一起制作词向量训练原始数据\n",
    "\n",
    "#x_all = list(train_x)+list(dev_x)\n",
    "#print(\"长度检查：\",len(train_x)+len(dev_x)==len(x_all))#检查长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n"
     ]
    }
   ],
   "source": [
    "#分词\n",
    "x_all_cut=[]#分词后的语料库\n",
    "for i in range(len(x_all)):\n",
    "    x_all_cut.append(word_cut(x_all[i]))\n",
    "print(len(x_all_cut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#使用gensim训练词向量并保存至本地\n",
    "word_model = gensim.models.Word2Vec(x_all_cut,min_count=1)\n",
    "word_model.save(\"word_model/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fgd/.conda/envs/keras-nlp/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5148908"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#加载并测试词向量\n",
    "#word_model[\"人性化\"]\n",
    "word_model.similarity(\"亮点\",\"特性\")#计算词汇相似度测试\n",
    "#word_model.n_similarity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fgd/.conda/envs/keras-nlp/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_model[\"儿子\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def build_sentence_vector(sentence,size,w2v_model):\n",
    "    sen_vec=np.zeros(size).reshape((1,size))\n",
    "    count=0\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            sen_vec+=w2v_model[word].reshape((1,size))\n",
    "            count+=1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count!=0:\n",
    "        sen_vec/=count\n",
    "    return sen_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fgd/.conda/envs/keras-nlp/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# result = build_sentence_vector(train_x[0],100,word_model)\n",
    "# print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_id2wec(model):\n",
    "    gensim_dict = gensim.corpora.dictionary.Dictionary()\n",
    "    gensim_dict.doc2bow(model.wv.vocab.keys(), allow_update=True)\n",
    "    w2id = {v: k + 1 for k, v in gensim_dict.items()}  # 词语的索引，从1开始编号\n",
    "    w2vec = {word: model[word] for word in w2id.keys()}  # 词语的词向量\n",
    "    n_vocabs = len(w2id) + 1\n",
    "    embedding_weights = np.zeros((n_vocabs, 100))\n",
    "    for w, index in w2id.items():  # 从索引为1的词语开始，用词向量填充矩阵\n",
    "        embedding_weights[index, :] = w2vec[w]\n",
    "    return w2id,embedding_weights\n",
    "\n",
    "\n",
    "def text_to_array(w2index, senlist):  # 文本转为索引数字模式\n",
    "    sentences_array = []\n",
    "    for sen in senlist:\n",
    "        new_sen = [ w2index.get(word,0) for word in sen]   # 单词转索引数字\n",
    "        sentences_array.append(new_sen)\n",
    "    return np.array(sentences_array)\n",
    "\n",
    "\n",
    "def prepare_data(w2id,sentences,labels,max_len=100):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(sentences,labels, test_size=0.2)\n",
    "    X_train = text_to_array(w2id, X_train)\n",
    "    X_val = text_to_array(w2id, X_val)\n",
    "    X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "    X_val = keras.preprocessing.sequence.pad_sequences(X_val, maxlen=max_len)\n",
    "    return np.array(X_train), keras.utils.np_utils.to_categorical(y_train) ,np.array(X_val), keras.utils.np_utils.to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fgd/.conda/envs/keras-nlp/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "w2id,embedding_weights = generate_id2wec(word_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "sentences = list(train_x)+list(dev_x)\n",
    "labels = list(train_y)+list(dev_y)\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = word_cut(sentences[i])\n",
    "sentences[:5]\n",
    "x_train,y_train, x_val , y_val = prepare_data(w2id,sentences,labels,200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 200, 100)          3735700   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,796,302\n",
      "Trainable params: 3,796,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional,LSTM,Dense,Embedding,Dropout,Activation,Softmax\n",
    "\n",
    "class Sentiment:\n",
    "    def __init__(self,w2id,embedding_weights,Embedding_dim,maxlen,labels_category):\n",
    "        self.Embedding_dim = Embedding_dim\n",
    "        self.embedding_weights = embedding_weights\n",
    "        self.vocab = w2id\n",
    "        self.labels_category = labels_category#类别数量\n",
    "        self.maxlen = maxlen\n",
    "        self.model = self.build_model()#模型通过build_model方法来构建\n",
    "      \n",
    "        \n",
    "    def build_model(self):\n",
    "        model = keras.Sequential()\n",
    "        #input dim(140,100)\n",
    "        model.add(Embedding(output_dim = self.Embedding_dim,\n",
    "                           input_dim=len(self.vocab)+1,\n",
    "                           weights=[self.embedding_weights],\n",
    "                           input_length=self.maxlen))\n",
    "        model.add(Bidirectional(LSTM(50),merge_mode='concat'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(self.labels_category))\n",
    "        model.add(Activation('softmax'))#relu\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                     optimizer='adam', \n",
    "                     metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    " \n",
    "    def train(self,X_train, y_train,X_test, y_test,n_epoch=5 ):\n",
    "        self.model.fit(X_train, y_train, batch_size=32, epochs=n_epoch,\n",
    "                      validation_data=(X_test, y_test))\n",
    "        self.model.save('sentiment.h5')   \n",
    "        \n",
    " \n",
    "    def predict(self,model_path,new_sen):\n",
    "        model = self.model\n",
    "        model.load_weights(model_path)\n",
    "        new_sen_list = jieba.lcut(new_sen)\n",
    "        sen2id =[ self.vocab.get(word,0) for word in new_sen_list]\n",
    "        sen_input = keras.preprocessing.sequence.pad_sequences([sen2id], maxlen=self.maxlen)\n",
    "        res = model.predict(sen_input)[0]\n",
    "        return np.argmax(res)\n",
    " \n",
    " \n",
    "senti = Sentiment(w2id,embedding_weights,100,200,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fgd/.conda/envs/keras-nlp/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10640 samples, validate on 2660 samples\n",
      "Epoch 1/5\n",
      "10640/10640 [==============================] - 62s 6ms/step - loss: 0.3028 - accuracy: 0.8765 - val_loss: 0.2086 - val_accuracy: 0.9132\n",
      "Epoch 2/5\n",
      "10640/10640 [==============================] - 58s 5ms/step - loss: 0.1777 - accuracy: 0.9315 - val_loss: 0.1926 - val_accuracy: 0.9226\n",
      "Epoch 3/5\n",
      "10640/10640 [==============================] - 59s 6ms/step - loss: 0.1224 - accuracy: 0.9554 - val_loss: 0.1758 - val_accuracy: 0.9267\n",
      "Epoch 4/5\n",
      "10640/10640 [==============================] - 60s 6ms/step - loss: 0.0833 - accuracy: 0.9702 - val_loss: 0.1827 - val_accuracy: 0.9320\n",
      "Epoch 5/5\n",
      "10640/10640 [==============================] - 60s 6ms/step - loss: 0.0554 - accuracy: 0.9810 - val_loss: 0.2129 - val_accuracy: 0.9305\n"
     ]
    }
   ],
   "source": [
    "senti.train(x_train,y_train, x_val ,y_val,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti.predict(\"./sentiment.h5\",\"不太行\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}